{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fedflax import train\n",
    "from models import ResNet, ResNetAutoEncoder\n",
    "from data import fetch_data\n",
    "from utils import load_model, return_ce, mean_iou_err\n",
    "import jax, optax, pickle\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf02fb",
   "metadata": {},
   "source": [
    "## Fetch foundation model\n",
    "Trained using the imagenet script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload backbone and use as encoder\n",
    "models = load_model(\n",
    "    lambda: ResNet(layers=[2,2,2,2], dim_out=100), \n",
    "    \"models/resnet18_central_imagenet100.pkl\"\n",
    ")\n",
    "struct, params, rest = nnx.split(models, (nnx.Param, nnx.BatchStat), ...)\n",
    "model = nnx.merge(\n",
    "    struct,\n",
    "    jax.tree.map(lambda p: p.mean(0), params),\n",
    "    rest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804e212",
   "metadata": {},
   "source": [
    "## Alternatively, fetch ViT-224 foundation model\n",
    "Requires installing https://github.com/google-research/vision_transformer.\n",
    "\n",
    "The weights are available at https://console.cloud.google.com/storage/browser/vit_models/imagenet21k. Any version should do, if you change the config accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_jax.models_vit import VisionTransformer\n",
    "from functools import partial\n",
    "from ml_collections.config_dict import ConfigDict\n",
    "from packaging import version\n",
    "import flax, sys\n",
    "# This is a workaround to import the `load_pretrained` without installing tensorflow\n",
    "flax.io.gfile = flax.io\n",
    "sys.modules[\"tensorflow.io\"] = flax.io\n",
    "from vit_jax.checkpoint import load, inspect_params, _fix_groupnorm\n",
    "\n",
    "# Config copied from the ViT-B_16 at https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py#L113\n",
    "config = ConfigDict({\n",
    "    \"num_classes\": 0, # No classification head\n",
    "    \"patches\": ConfigDict({\"size\": (16, 16)}),\n",
    "    \"model_name\": \"ViT-B_16\",\n",
    "    \"transformer\": ConfigDict(\n",
    "        {\"mlp_dim\": 3072, \"num_heads\": 12, \"num_layers\": 12, \"attention_dropout_rate\": 0.0, \"dropout_rate\": 0.0}\n",
    "    ),\n",
    "    \"classifier\": \"token\",\n",
    "    \"representation_size\": None,\n",
    "    \"hidden_size\": 768\n",
    "})\n",
    "model = VisionTransformer(**config)\n",
    "reference_params = model.init(jax.random.key(42), jnp.ones((1,224,224,3)), train=False)[\"params\"]\n",
    "# Dumbed down version of `load_pretrained`\n",
    "# Head is explicitly removed\n",
    "# Case where posemb_new.shape!=posemb.shape is not handled\n",
    "params = _fix_groupnorm(inspect_params(\n",
    "    params=load(\"models/ViT-B_16.npz\"),\n",
    "    expected=reference_params,\n",
    "    fail_if_extra=False,\n",
    "    fail_if_missing=False))\n",
    "if config.get(\"representation_size\") is None and \"pre_logits\" in params:\n",
    "    params[\"pre_logits\"] = {}\n",
    "if version.parse(flax.__version__) >= version.parse(\"0.3.6\"):\n",
    "    params = _fix_groupnorm(params)\n",
    "params.pop(\"head\")\n",
    "params = flax.core.freeze(params)\n",
    "# Define inference function\n",
    "infer_fn = jax.jit(partial(model.apply, train=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad6551",
   "metadata": {},
   "source": [
    "## Finetune using asymmetries\n",
    "Compare with and without asymmetries in the ResNetAutoEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes data\n",
    "n_clients = 3\n",
    "ds_train = fetch_data(beta=1., dataset=2, n_clients=n_clients, batch_size=32)\n",
    "ds_val = fetch_data(beta=1., dataset=2, partition=\"val\", n_clients=n_clients, batch_size=16)\n",
    "\n",
    "# Autoencoder model for segmentation via image reconstruction\n",
    "asymkwargs = {}\n",
    "ae = ResNetAutoEncoder(backboneencoder=model, key=jax.random.key(43), **asymkwargs)\n",
    "\n",
    "# Optimizer with lower lr for pretrained backbone\n",
    "lr = optax.warmup_exponential_decay_schedule(1e-4, .1, 4000, 1000, .9, end_value=1e-5)\n",
    "def opt_create(ae:ResNetAutoEncoder):\n",
    "    return nnx.Optimizer(\n",
    "        ae,\n",
    "        optax.chain(\n",
    "            optax.masked(optax.adamw(1e-4), lambda ptree: jax.tree.map_with_path(lambda path, _p: \"backboneencoder\" in path, ptree)),\n",
    "            optax.masked(optax.adamw(lr), lambda ptree: jax.tree.map_with_path(lambda path, _p: not \"backboneencoder\" in path, ptree))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Train\n",
    "aes, rounds = train(\n",
    "    ae,\n",
    "    opt_create,\n",
    "    ds_train,\n",
    "    return_ce(0.), \n",
    "    ds_val,\n",
    "    local_epochs=\"early\",\n",
    "    n_clients=n_clients,\n",
    "    max_patience=3,\n",
    "    rounds=\"early\",\n",
    "    val_fn=mean_iou_err\n",
    ")\n",
    "\n",
    "# Save decoder\n",
    "state = nnx.state(aes, nnx.Not(nnx.PathContains(\"backboneencoder\")))\n",
    "pickle.dump(state, open(\"models/cs_rn18_decoder.pkl\", \"wb\"))\n",
    "\n",
    "# Reload and aggregate it\n",
    "load_fn = lambda: ResNetAutoEncoder(\n",
    "    backboneencoder=load_model(\n",
    "        lambda: ResNet(layers=[2,2,2,2], dim_out=100), \n",
    "        \"models/resnet18_central_imagenet100.pkl\"\n",
    "    ),\n",
    "    **asymkwargs\n",
    ")\n",
    "aes = load_model(load_fn, \"models/cs_rn18_autoencoder.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
