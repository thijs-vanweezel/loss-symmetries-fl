{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from jax import config\n",
    "config.update(\"jax_platforms\", \"cpu\")\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_ops=true\"\n",
    "import jax, optax, pickle\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "from fedflax import train\n",
    "from models import LeNet\n",
    "from data import fetch_data\n",
    "from utils import opt_create, return_l2\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4387688",
   "metadata": {},
   "source": [
    "## Bezier interpolation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bezier function\n",
    "def bezier(t):\n",
    "    def inner(theta1, theta2, w):\n",
    "        return (1 - t) ** 2 * theta1 + 2 * (1 - t) * t * w + t ** 2 * theta2\n",
    "    return inner\n",
    "\n",
    "# Loss is calculated using the model resulting from a Bezier interpolation\n",
    "def ell_tilde(w, x, y, key):\n",
    "    t = jax.random.uniform(key, ()) # TODO: should actually differ per sample\n",
    "    theta = jax.tree.map(bezier(t), theta1, theta2, w)\n",
    "    model = nnx.merge(struct, theta, rest)\n",
    "    return optax.softmax_cross_entropy(model(x), y).mean()\n",
    "\n",
    "# Straightforward\n",
    "@jax.jit\n",
    "def train_step(w, x, y, opt_state, key):\n",
    "    loss, grad = nnx.value_and_grad(ell_tilde)(w, x, y, key)\n",
    "    updates, opt_state = opt.update(grad, opt_state, w)\n",
    "    w = optax.apply_updates(w, updates)\n",
    "    return loss, opt_state, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af76f3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(10):\n",
    "    # Train model\n",
    "    model = LeNet(jax.random.key(seed))\n",
    "    opt = opt_create(model, learning_rate=1e-3)\n",
    "    ds_train = fetch_data(skew=\"feature\", beta=1., n_clients=2)\n",
    "    ds_val = fetch_data(skew=\"feature\", beta=1., partition=\"val\", batch_size=16, n_clients=2)\n",
    "    ds_test = fetch_data(skew=\"feature\", beta=1., partition=\"test\", batch_size=16, n_clients=2)\n",
    "    _, models = train(model, opt, ds_train, return_l2(0.), ds_val, local_epochs=\"early\", n_clients=2, max_patience=4, rounds=\"early\")\n",
    "\n",
    "    # Get client models\n",
    "    struct, theta, rest = nnx.split(models, (nnx.Param, nnx.BatchStat), ...)\n",
    "    theta1 = jax.tree.map(lambda p: p[0], theta)\n",
    "    theta2 = jax.tree.map(lambda p: p[1], theta)\n",
    "\n",
    "    # Bezier parameter and optimizer\n",
    "    key = jax.random.key(seed)\n",
    "    def rand_like(x): global key; key, subkey = jax.random.split(key); return jax.random.normal(subkey, shape=x.shape)\n",
    "    w = jax.tree.map(rand_like, theta1)\n",
    "    opt = optax.adamw(learning_rate=1e-3)\n",
    "    opt_state = opt.init(w)\n",
    "\n",
    "    # Optimize Bezier curve\n",
    "    for epoch in tqdm(range(20)):\n",
    "        # Concatenate both clients' data\n",
    "        for x_batch, y_batch in ds_train:\n",
    "            loss, opt_state, w = train_step(w, x_batch, y_batch, opt_state, key)\n",
    "            _, key = jax.random.split(key)\n",
    "    \n",
    "    pickle.dump((theta1, theta2, w), open(f\"bezier_model_seed{seed}.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
