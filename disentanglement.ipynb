{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d83bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fedflax import train\n",
    "from models import ResNet\n",
    "from data import fetch_data\n",
    "from utils import return_ce, top_5_err, nnx_norm\n",
    "import jax, json, optax\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "from matplotlib import pyplot as plt\n",
    "from npy_append_array import NpyAppendArray\n",
    "n_clients = 3\n",
    "\n",
    "# Matplotlib rendering style\n",
    "plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "    \"font.sans-serif\": [\"Helvetica\"],\n",
    "    \"text.latex.preamble\": r\"\"\"\n",
    "        \\usepackage{amsmath, amssymb}\n",
    "        \\usepackage{mathptmx}\n",
    "    \"\"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a782d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(42)\n",
    "vcall = nnx.jit(nnx.vmap(\n",
    "    lambda model, x: model(x, train=False), \n",
    "    in_axes=(0, None)\n",
    "))\n",
    "\n",
    "# Sharpness-aware minimization optimizer\n",
    "def sam(model): return nnx.Optimizer(\n",
    "    model,\n",
    "    optax.contrib.sam(\n",
    "        optax.sgd(learning_rate=lr, momentum=.9),\n",
    "        optax.chain(optax.contrib.normalize(), optax.sgd(.1)),\n",
    "        sync_period=2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "# Induce different distances via elongated FL training\n",
    "inducers = jnp.arange(20).tolist()\n",
    "# Store logits to compute entanglement scores (shape: n_inducers*n_samples, n_clients or none, n_partitions, n_classes)\n",
    "jnp.save(\"disentanglement/labels.npy\", jnp.empty((0,n_clients,100)))\n",
    "jnp.save(\"disentanglement/logits_l.npy\", jnp.empty((0,n_clients,n_clients,100)))\n",
    "jnp.save(\"disentanglement/logits_g.npy\", jnp.empty((0,n_clients,100)))\n",
    "jnp.save(\"disentanglement/logits_l_rand.npy\", jnp.empty((0,n_clients,n_clients,100)))\n",
    "jnp.save(\"disentanglement/logits_g_rand.npy\", jnp.empty((0,n_clients,100)))\n",
    "distances = []\n",
    "# Initialize global model\n",
    "model_g = ResNet(key, dim_out=100)\n",
    "lr = optax.warmup_exponential_decay_schedule(1e-4, 1., 200, 100, .9, end_value=1e-5)\n",
    "opt = nnx.Optimizer( # sam(model_g)\n",
    "    model_g,\n",
    "    optax.adam(learning_rate=lr),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "for inducer in inducers:\n",
    "    # Get datasets which induce this distance\n",
    "    ds_train = fetch_data(\"label\", beta=1., n_clients=n_clients, dataset=1, n_classes=100)\n",
    "    ds_test = fetch_data(\"label\", partition=\"test\", beta=1., batch_size=64, n_clients=n_clients, dataset=1, n_classes=100)\n",
    "    ds_val = fetch_data(\"label\", partition=\"val\", beta=1., batch_size=64, n_clients=n_clients, dataset=1, n_classes=100)\n",
    "    # Get federated models which are this far apart\n",
    "    fl_models, _ = train(model_g, opt, \n",
    "                      ds_train, return_ce(0.), ds_val, local_epochs=\"early\", \n",
    "                      max_patience=3, val_fn=top_5_err, rounds=1, n_clients=n_clients)\n",
    "    # Check entanglement wrt global model\n",
    "    struct, params_l, batchstat_l, rest = nnx.split(fl_models, nnx.Param, nnx.BatchStat, ...)\n",
    "    params_g = jax.tree.map(lambda p: p.mean(0), params_l)\n",
    "    batchstat_g = jax.tree.map(lambda b: b.mean(0), batchstat_l)\n",
    "    model_g = nnx.merge(struct, params_g, batchstat_g, rest)\n",
    "    # Get L2 distance\n",
    "    distance = nnx_norm(params_l, params_g, order=2., n_clients=n_clients).mean()\n",
    "    # Get models which are (measured across all params) this far apart at random\n",
    "    # TODO: questions 1) should the random global model be further randomized; 2) which batchstats should the random local models receive?\n",
    "    rand_model_g = ResNet(key, dim_out=100)\n",
    "    rstate, rparams_g, *_ = nnx.split(rand_model_g, nnx.Param, nnx.BatchStat, ...)\n",
    "    cpkey = jax.random.key(42)\n",
    "    def random_like(p): global cpkey; _, cpkey = jax.random.split(cpkey); return jax.random.normal(cpkey, (n_clients,*p.shape))\n",
    "    rand = jax.tree.map(random_like, rparams_g)\n",
    "    norm = nnx_norm(rand, n_clients=n_clients, order=2.)\n",
    "    rparams_l = jax.tree.map(lambda p, r: p+(r.T/norm).T*distance, rparams_g, rand)\n",
    "    rand_models = nnx.merge(rstate, rparams_l, batchstat_l, rest)\n",
    "\n",
    "    # Now store logits to calculate entanglement\n",
    "    distances.append(distance.item())\n",
    "    json.dump(distances, open(\"disentanglement/distances.json\", \"w\"))\n",
    "    for vy, vx in ds_test:\n",
    "        # Reshape so that we get the logits of each client over all partitions\n",
    "        flaty = vy.reshape(-1, *vy.shape[2:])\n",
    "        flatx = vx.reshape(-1, *vx.shape[2:])\n",
    "        out_shape = (n_clients, n_clients, -1, 100)\n",
    "        # Append out-of-memory\n",
    "        with (NpyAppendArray(\"disentanglement/labels.npy\") as labels,\n",
    "              NpyAppendArray(\"disentanglement/logits_l.npy\") as logits_l,\n",
    "              NpyAppendArray(\"disentanglement/logits_g.npy\") as logits_g,\n",
    "              NpyAppendArray(\"disentanglement/logits_l_rand.npy\") as logits_l_rand,\n",
    "              NpyAppendArray(\"disentanglement/logits_g_rand.npy\") as logits_g_rand):\n",
    "            labels.append(vy.swapaxes(0,-2).__array__())\n",
    "            logits_l.append(vcall(fl_models, flatx).reshape(out_shape).swapaxes(0,-2).__array__())\n",
    "            logits_g.append(model_g(flatx, train=False).reshape(out_shape[1:]).swapaxes(0,-2).__array__())\n",
    "            logits_l_rand.append(vcall(rand_models, flatx).reshape(out_shape).swapaxes(0,-2).__array__())\n",
    "            logits_g_rand.append(rand_model_g(flatx, train=False).reshape(out_shape[1:]).swapaxes(0,-2).__array__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e910da",
   "metadata": {},
   "source": [
    "## Visualize entanglement\n",
    "Entanglement score: $\\sum_{i=0}^{|\\mathcal{C}|} \\mathbb{E}_{x\\sim\\mathcal{D}_i} [\\text{dist}( f(\\theta_i, x), f(\\bar{\\theta}, x) )]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in memory\n",
    "logits_l = jnp.load(\"disentanglement/logits_l.npy\").reshape(len(inducers), -1, n_clients, n_clients, 100)\n",
    "logits_g = jnp.load(\"disentanglement/logits_g.npy\").reshape(len(inducers), -1, n_clients, 100)\n",
    "logits_l_rand = jnp.load(\"disentanglement/logits_l_rand.npy\").reshape(len(inducers), -1, n_clients, n_clients, 100)\n",
    "logits_g_rand = jnp.load(\"disentanglement/logits_g_rand.npy\").reshape(len(inducers), -1, n_clients, 100)\n",
    "distances = json.load(open(\"disentanglement/distances.json\", \"r\"))\n",
    "# Calculate entanglement per round\n",
    "entfn = lambda ll, lg: jnp.abs((ll-ll.min())/jnp.sum(ll-ll.min(), -1, keepdims=True) - (lg-lg.min())/jnp.sum(lg-lg.min(), -1, keepdims=True)).sum(-1).mean().item()/2\n",
    "fl_entanglement_scores = [\n",
    "        sum(entfn(logits_l[i,:,c,c,:], logits_g[i,:,c,:]) for c in range(n_clients))/n_clients\n",
    "    for i in range(len(inducers))]\n",
    "rand_entanglement_scores = [\n",
    "        sum(entfn(logits_l_rand[i,:,c,c,:], logits_g_rand[i,:,c,:]) for c in range(n_clients))/n_clients\n",
    "    for i in range(len(inducers))]\n",
    "\n",
    "# Plot entanglement vs distance\n",
    "fig, ax = plt.subplots();\n",
    "\n",
    "ax.plot(inducers[:8], fl_entanglement_scores[:8], label=\"FL models\", c=\"C0\");\n",
    "ax.plot(inducers[:8], rand_entanglement_scores[:8], label=\"Random models\", c=\"C1\");\n",
    "ax.set_xticks(inducers[:8], labels=map(lambda x: f\"{x:.1f}\", distances[:8]), rotation=45);\n",
    "ax.set_xlabel(\"L2 client drift\");\n",
    "ax.set_xlim(min(inducers[:8]), max(inducers[:8]));\n",
    "ax.grid(True, linestyle=\"--\", linewidth=0.5, axis=\"x\");\n",
    "ax.set_ylim(0.);\n",
    "\n",
    "ax2 = ax.twiny();\n",
    "ax2.spines.top.set_visible(False);\n",
    "ax2.xaxis.set_ticks_position(\"bottom\");\n",
    "ax2.xaxis.set_label_position(\"bottom\");\n",
    "ax2.spines.bottom.set_position((\"outward\", 50));\n",
    "ax2.set_xlim(min(inducers[:8]), max(inducers[:8]));\n",
    "ax2.set_xticks(inducers[:8], labels=map(int, list(inducers[:8])));\n",
    "ax2.set_xlabel(\"Communication round\");\n",
    "\n",
    "fig.legend();\n",
    "fig.supylabel(\"Functional entanglement score\");\n",
    "fig.savefig(\"disentanglement/disentanglement_fedavg.png\", dpi=300, bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2ce12",
   "metadata": {},
   "source": [
    "## Plot function over labels (ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3792b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logits and merge partitions\n",
    "logits_l = jnp.load(\"disentanglement/logits_l.npy\").swapaxes(1,2).reshape(len(inducers), -1, n_clients, 100)\n",
    "logits_g = jnp.load(\"disentanglement/logits_g.npy\").reshape(len(inducers), -1, 100)\n",
    "labels = jnp.load(\"disentanglement/labels.npy\").argmax(-1).reshape(len(inducers), -1)\n",
    "ulabels = jnp.unique(labels).sort()\n",
    "\n",
    "# Function that returns the best guess out of top 5\n",
    "best_of_5 = lambda label, lgts: label if label in jnp.argsort(lgts, axis=-1)[-5:] else jnp.argmax(lgts, axis=-1)\n",
    "# Function that returns the amount by which the correct label is displaced in the sorted logits (which are averaged over samples)\n",
    "displacement = lambda label, lgts: jnp.argmin(jnp.abs(jnp.argsort(lgts, axis=-1, descending=True).mean(-2)-label), axis=-1)\n",
    "# Plot\n",
    "fig, ax = plt.subplots();\n",
    "for i in range(n_clients):\n",
    "    # This indexation says: last round, samples belonging to label, client i, best prediction, mean over samples\n",
    "    scoreperlabel = [best_of_5(label, logits_l[-1, labels[-1]==label, i, :].mean(0)) for label in ulabels]\n",
    "    # This indexation says: all rounds, samples belonging to label, client i, mean displacement over rounds\n",
    "    disperlabel = [displacement(label, logits_l[:,labels[-1]==label,i,:]).mean(0) for label in ulabels] + [0]\n",
    "    # Plot mean line with shaded variation (normalized by 2 for polarity and 5 for visibility)\n",
    "    ax.hlines(scoreperlabel, ulabels-1, ulabels, colors=\"C\"+str(i), label=f\"Client {i}\")\n",
    "    ax.fill_between(jnp.arange(0,100),\n",
    "                    jnp.array(scoreperlabel+[0])-jnp.array(disperlabel)/2/5,\n",
    "                    jnp.array(scoreperlabel+[0])+jnp.array(disperlabel)/2/5,\n",
    "                    color=\"C\"+str(i), alpha=.5, step=\"post\", linewidth=0)\n",
    "    # Show domain\n",
    "    ax.axvspan(i*len(ulabels)/n_clients, (i+1)*len(ulabels)/n_clients, color=\"C\"+str(i), alpha=.18, zorder=0)\n",
    "\n",
    "# Same for aggregated model\n",
    "scoreperlabel = [best_of_5(label, logits_g[-1,labels[-1]==label,:].mean(0)) for label in ulabels]\n",
    "disperlabel = [displacement(label, logits_g[:,labels[-1]==label,:]).mean(0) for label in ulabels] + [0]\n",
    "ax.hlines(scoreperlabel, ulabels-1, ulabels, colors=\"black\", label=\"Aggregated model\")\n",
    "ax.fill_between(jnp.arange(0,100),\n",
    "                jnp.array(scoreperlabel+[0])-jnp.array(disperlabel)/2/5,\n",
    "                jnp.array(scoreperlabel+[0])+jnp.array(disperlabel)/2/5,\n",
    "                color=\"black\", alpha=.3, step=\"post\", linewidth=0)\n",
    "\n",
    "# # ax.plot(labels, labels - outs_init, c=\"gray\", label=\"Ideal model\");\n",
    "# # ax.axhline(0., c=\"black\", linestyle=\"-.\", label=\"Relative initialization\");\n",
    "fig.legend(framealpha=1);\n",
    "ax.set_xlabel(\"Label\");\n",
    "ax.set_ylabel(r\"$\\text{argmax}\\mathbb{E}_{x\\in\\mathcal{D}_{\\text{label}}}[f(\\theta_i, x)]$\")\n",
    "ax.set_xlim(0, 99);\n",
    "ax.set_ylim(0, 99);\n",
    "ax.set_xticks(jnp.arange(0,101,10).at[-1].set(99), labels=[0]+[None]*4+[\"...\"]+[None]*4+[99], minor=False)\n",
    "ax.set_yticks(jnp.arange(0,101,10).at[-1].set(99), labels=[0]+[None]*4+[\"...\"]+[None]*4+[99], minor=False, rotation=90)\n",
    "ax.set_yticks(jnp.arange(0, 100), minor=True)\n",
    "ax.set_xticks(jnp.arange(0, 100), minor=True)\n",
    "ax.grid(True, linestyle=\"--\", linewidth=0.5, which=\"major\");\n",
    "fig.savefig(\"disentanglement/function_fedavg.png\", dpi=300, bbox_inches=\"tight\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
