{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import optax, jax\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "from fedflax import train\n",
    "from models import ResNet\n",
    "from data import get_gaze\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9db9f5",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpness-aware minimization optimizer\n",
    "sam = lambda model: nnx.Optimizer(\n",
    "    model,\n",
    "    optax.contrib.sam(\n",
    "        optax.adamw(learning_rate=1e-3),\n",
    "        optax.chain(optax.contrib.normalize(), optax.adam(1e-2)),\n",
    "        sync_period=5\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "opt_reg = lambda model: nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(learning_rate=1e-3),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "# Loss includes softmax layer\n",
    "def ell(model, _, x_batch, z_batch, y_batch, train):\n",
    "    ce = optax.softmax_cross_entropy(model(x_batch, z_batch, train=train), y_batch).mean()\n",
    "    return ce, (0., 0.)\n",
    "\n",
    "def partial_aggregate(models, alpha):\n",
    "    params, struct = jax.tree.flatten(nnx.to_tree(models))\n",
    "    avg_params = jax.tree.map(lambda p: jnp.mean(p, axis=0), params)\n",
    "    new_params = jax.tree.map(lambda p, ap: (1-alpha)*p + alpha*ap, params, avg_params)\n",
    "    return nnx.from_tree(jax.tree.unflatten(struct, new_params))\n",
    "\n",
    "# Calculate the dominant eigenvalue (lambda_max) of an nnx model using the power iteration method\n",
    "key = jax.random.key(42)\n",
    "def lambda_max(model, x, y, max_iter=20, key=key):\n",
    "    # Convenience\n",
    "    theta, struct = jax.tree.flatten(model)\n",
    "    shapes = [jnp.array(p.shape) for p in theta]\n",
    "    sections = jnp.cumsum(jnp.array([jnp.prod(shape) for shape in shapes]))\n",
    "    inflate = lambda th: nnx.from_tree(jax.tree.unflatten(struct,\n",
    "        [layer.reshape(s) for layer,s in zip(jnp.split(th, sections), shapes)]))\n",
    "    # Cross entropy\n",
    "    ce_fn = lambda m: optax.losses.softmax_cross_entropy(m(x), y).mean()\n",
    "    # Get first gradient\n",
    "    flat_grad_fn = lambda theta: jnp.concatenate([jnp.ravel(g) for g in jax.tree.leaves(\n",
    "        nnx.grad(ce_fn)(inflate(theta))\n",
    "    )])\n",
    "    # Random normalized vector\n",
    "    theta = jnp.concatenate([jnp.ravel(layer) for layer in theta])\n",
    "    v = jax.random.normal(key, theta.shape)\n",
    "    v = v / (jnp.linalg.norm(v)+1e-9)\n",
    "    # Power iteration \n",
    "    for _ in range(max_iter):\n",
    "        hv = jax.jvp(\n",
    "            flat_grad_fn,\n",
    "            (theta,),\n",
    "            (v,)\n",
    "        )[1]\n",
    "        v = hv / (jnp.linalg.norm(hv)+1e-9)\n",
    "    # Return Rayleigh quotient\n",
    "    return jnp.dot(v, flat_grad_fn(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c04db",
   "metadata": {},
   "source": [
    "## Interpolate between client and global, and measure metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get models after one-shot for various heterogeneity levels\n",
    "metrics = defaultdict(lambda: defaultdict(list))\n",
    "for opt_create, beta, skew in [(opt_reg, 0., \"feature\"), (opt_reg, .5, \"feature\"), (opt_reg, 1., \"feature\"), (opt_reg, 0., \"overlap\"), (opt_reg, .5, \"overlap\"), (opt_reg, 1., \"overlap\"), (sam, 0., \"feature\"), (sam, .5, \"feature\"), (sam, 1., \"feature\")]:\n",
    "    # Train model\n",
    "    ds_train = get_gaze(skew=\"feature\", beta=beta)\n",
    "    ds_val = get_gaze(skew=\"feature\", beta=beta, partition=\"val\", batch_size=16)\n",
    "    ds_test = get_gaze(skew=\"feature\", beta=beta, partition=\"test\", batch_size=16)\n",
    "    _, models = train(ResNet, opt_create, ds_train, ds_val, ell, local_epochs=50, max_rounds=0)\n",
    "    # Interpolate\n",
    "    for alpha in tqdm(jnp.linspace(0,1,30), leave=False):\n",
    "        # Get the client models between global and local at alpha\n",
    "        models = partial_aggregate(models, alpha)\n",
    "        # Average accuracy on local data\n",
    "        acc_fn = nnx.jit(nnx.vmap(lambda m,x,z,y: (m(x,z,train=False).argmax(-1)==y.argmax(-1))))\n",
    "        err_g = 1 - reduce(lambda acc, b: acc + acc_fn(models,*b), ds_test, 0.) / len(ds_test)\n",
    "        metrics[f\"{skew}_{beta}_{opt_create.__name__}\"][\"Avg. loc. err.\"].append(err_g)\n",
    "        # Average accuracy on global data\n",
    "        acc_fn = nnx.jit(nnx.vmap(lambda m,x,z,y: (m(x.reshape(-1,36,60,1),z.reshape(-1,3),train=False).argmax(-1)==y.reshape(-1,16).argmax(-1)), in_axes=(0,None,None,None)))\n",
    "        accs_l = 1 - reduce(lambda acc, b: acc + acc_fn(models,*b), ds_test, 0.) / len(ds_test)\n",
    "        metrics[f\"{skew}_{beta}_{opt_create.__name__}\"][\"Avg. glob. err.\"].append(accs_l)\n",
    "        # # Average top Hessian eigenvalue\n",
    "        # lambda_maxs = nnx.vmap(lambda_max, in_axes=(0,None,None,None,None))(models, x_test, jnp.eye(10)[y_test], 5, key)\n",
    "        # metrics[beta][r\"Avg. $\\lambda_\\text{max}$\"].append(lambda_maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811288f",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "for name in metrics.keys(): \n",
    "    fig, axs = plt.subplots(dpi=300)\n",
    "    twin1 = axs.twinx()\n",
    "    twin2 = axs.twinx()\n",
    "    twin2.spines.right.set_position((\"axes\", 1.2))\n",
    "    for c, (metric, ax) in enumerate(zip(metrics[name], [axs, twin1, twin2])):\n",
    "        data = metrics[name][metric]\n",
    "        ax.plot(jnp.linspace(0,1,len(data)), data.mean(axis=-1), c=f\"C{c}\", label=metric)\n",
    "        if metric != \"Avg. loc. acc.\":\n",
    "            ax.fill_between(jnp.linspace(0,1,len(data)), data.min(axis=-1), data.max(axis=-1), color=f\"C{c}\", alpha=.2)\n",
    "        ax.set_ylabel(metric, color=f\"C{c}\")\n",
    "        ax.tick_params(axis=\"y\", colors=f\"C{c}\")\n",
    "    axs.set_xlabel(\"Interpolation coefficient\")\n",
    "    axs.set_xlim(0,1)\n",
    "    # fig.legend()\n",
    "    fig.savefig(f\"agg/interpolation_{name}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
