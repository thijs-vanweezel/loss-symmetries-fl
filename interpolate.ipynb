{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_ops=true\"\n",
    "import optax, jax, json\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "from fedflax import train\n",
    "from models import LeNet\n",
    "from data import fetch_data\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce\n",
    "from utils import return_l2, angle_err, nnx_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9db9f5",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpness-aware minimization optimizer\n",
    "def sam(model): return nnx.Optimizer(\n",
    "    model,\n",
    "    optax.contrib.sam(\n",
    "        optax.sgd(learning_rate=1e-3, momentum=.9),\n",
    "        optax.chain(optax.contrib.normalize(), optax.sgd(.1)),\n",
    "        sync_period=2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "# Regular optimizer\n",
    "def opt_reg(model): return nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(learning_rate=1e-3),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "def partial_aggregate(models, alpha):\n",
    "    params, struct = jax.tree.flatten(nnx.to_tree(models))\n",
    "    avg_params = jax.tree.map(lambda p: jnp.mean(p, axis=0), params)\n",
    "    new_params = jax.tree.map(lambda p, ap: (1-alpha)*p + alpha*ap, params, avg_params)\n",
    "    return nnx.from_tree(jax.tree.unflatten(struct, new_params))\n",
    "\n",
    "# Calculate the dominant eigenvalue (lambda_max) of an nnx model using the power iteration method\n",
    "@nnx.vmap(in_axes=(0,None,None,None,None,None))\n",
    "def lambda_max(model, x, z, y, key, max_iter=20):\n",
    "    # Convenience (jvp must act on a parameter-only pytree)\n",
    "    struct, theta, rest = nnx.split(model, (nnx.Param, nnx.BatchStat), ...)\n",
    "    reconstruct = lambda th: nnx.merge(struct, th, rest)\n",
    "\n",
    "    # Gradient of loss on this data\n",
    "    grad_fn = nnx.grad(lambda th: jnp.square(reconstruct(th)(x, z, train=True) - y).mean())\n",
    "\n",
    "    # Random normalized vector\n",
    "    def random_like(arr): nonlocal key; _, key = jax.random.split(key); return jax.random.normal(key, arr.shape)\n",
    "    rand = jax.tree.map(lambda p: random_like(p), theta)\n",
    "\n",
    "    # Perform iteration avoiding python bools\n",
    "    def true_fun(val):\n",
    "        # Power iteration step\n",
    "        hv_prev, *_, i = val\n",
    "        norm = nnx_norm(hv_prev)\n",
    "        v = jax.tree.map(lambda hv_: hv_ / norm, hv_prev)\n",
    "        hv = jax.jvp(\n",
    "            grad_fn,\n",
    "            (theta,),\n",
    "            (v,)\n",
    "        )[1]\n",
    "        return hv, hv_prev, v, i+1\n",
    "    def cond_fun(val):\n",
    "        # Check convergence\n",
    "        hv, hv_prev, _, i = val\n",
    "        return jnp.logical_or(i<2, jnp.logical_and(i<max_iter, nnx_norm(hv, hv_prev)>=1e-3))\n",
    "    # Iterate\n",
    "    i0 = jnp.array(0)\n",
    "    hv, _, v, _ = jax.lax.while_loop(cond_fun, true_fun, (rand, rand, rand, i0))\n",
    "    # Return Rayleigh quotient (manual dot product between hv and v)\n",
    "    return jax.tree.reduce(\n",
    "        lambda acc, prod: acc+prod, \n",
    "        jax.tree.map(lambda hv_, v_: jnp.sum(hv_*v_), hv, v)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c04db",
   "metadata": {},
   "source": [
    "## Interpolate between client and global, and measure metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get models after one-shot for various heterogeneity levels\n",
    "metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(list))) # setting -> metric -> alpha -> seed -> value per client\n",
    "for opt_fn, beta, skew in [(opt_reg, 1., \"feature\"), (opt_reg, 0., \"overlap\"), (sam, 1., \"feature\")]:\n",
    "    for seed in range(10):\n",
    "        key = jax.random.key(seed)\n",
    "        # Train model\n",
    "        ds_train = fetch_data(skew=skew, beta=beta)\n",
    "        ds_val = fetch_data(skew=skew, beta=beta, partition=\"val\", batch_size=16)\n",
    "        ds_test = fetch_data(skew=skew, beta=beta, partition=\"test\", batch_size=16)\n",
    "        model = LeNet(key)\n",
    "        models, _ = train(LeNet(key), opt_fn(model), ds_train, return_l2(0.), ds_val, local_epochs=\"early\", rounds=1, max_patience=5, val_fn=angle_err)\n",
    "        # Interpolate\n",
    "        for alpha in tqdm(jnp.linspace(0,1,30).tolist(), leave=False):\n",
    "            # Get the client models between global and local at alpha\n",
    "            models_agg = partial_aggregate(models, alpha)\n",
    "            # Accuracy on local data\n",
    "            err_fn = nnx.jit(nnx.vmap(angle_err))\n",
    "            errs_l = reduce(lambda acc, b: acc + err_fn(models_agg,*b), ds_test, 0.) / len(ds_test)\n",
    "            metrics[f\"{skew}_{beta}_{opt_fn.__name__}\"][\"Avg. loc. err.\"][alpha].append(errs_l.tolist()) \n",
    "            # Accuracy on global data\n",
    "            err_fn = nnx.jit(nnx.vmap(angle_err, in_axes=(0,None,None,None)))\n",
    "            errs_g = reduce(lambda acc, batch: acc + err_fn(models_agg,*jax.tree.map(lambda x: x.reshape(-1, *x.shape[2:]), batch)), ds_test, 0.) / len(ds_test)\n",
    "            metrics[f\"{skew}_{beta}_{opt_fn.__name__}\"][\"Avg. glob. err.\"][alpha].append(errs_g.tolist())\n",
    "            # Top Hessian eigenvalue\n",
    "            y_test, x_test, z_test = reduce(lambda acc, batch: \n",
    "                                            jax.tree.map(lambda a, x: jnp.concatenate([a, x.reshape(-1, *x.shape[2:])]), acc, batch), \n",
    "                                            ds_test, \n",
    "                                            (jnp.empty((0,2)), jnp.empty((0,36,60,1)), jnp.empty((0,3))))\n",
    "            lambda_maxs = lambda_max(models_agg, x_test, z_test, y_test, key)\n",
    "            metrics[f\"{skew}_{beta}_{opt_fn.__name__}\"][r\"Avg. $\\lambda_\\text{max}$\"][alpha].append(lambda_maxs.tolist())\n",
    "        # Save intermediate results\n",
    "        with open(\"agg/metrics.json\", \"w\") as f:\n",
    "            json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811288f",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering style\n",
    "plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "    \"font.sans-serif\": [\"Helvetica\"],\n",
    "    \"text.latex.preamble\": r\"\"\"\n",
    "        \\usepackage{amsmath, amssymb}\n",
    "        \\usepackage{mathptmx}  % Safe fallback for Times + math\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# Plot all settings\n",
    "with open(\"agg/metrics.json\", \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "for name in metrics.keys(): \n",
    "    # There are supposed to be three metrics per setting\n",
    "    fig, mainax = plt.subplots(dpi=300);\n",
    "    # Plot\n",
    "    glob = jnp.array(list(metrics[name][\"Avg. glob. err.\"].values()));\n",
    "    glob_mean = glob.mean(axis=(-2,-1))\n",
    "    glob_std = glob.mean(-1).std(-1)\n",
    "    mainax.plot(jnp.linspace(0,1,len(glob)), glob_mean, c=f\"C1\", label=\"Error on global data\");\n",
    "    mainax.fill_between(jnp.linspace(0,1,len(glob)), glob_mean - glob_std, glob_mean + glob_std, color=f\"C1\", alpha=.2, linewidth=0.);\n",
    "    \n",
    "    loc = jnp.array(list(metrics[name][\"Avg. loc. err.\"].values()));\n",
    "    loc_mean = loc.mean(axis=(-2,-1))\n",
    "    loc_std = loc.mean(-1).std(-1)\n",
    "    mainax.plot(jnp.linspace(0,1,len(loc)), loc_mean, c=f\"C0\", label=\"Error on local data\");\n",
    "    mainax.fill_between(jnp.linspace(0,1,len(loc)), loc_mean - loc_std, loc_mean + loc_std, color=f\"C0\", alpha=.2, linewidth=0.);\n",
    "    \n",
    "    twin = mainax.twinx();\n",
    "    hess = jnp.array(list(metrics[name][r\"Avg. $\\lambda_\\text{max}$\"].values()));\n",
    "    hess_mean = hess.mean(axis=(-2,-1))\n",
    "    hess_std = hess.mean(-1).std(-1)\n",
    "    twin.plot(jnp.linspace(0,1,len(hess)), hess_mean, c=f\"C2\", label=r\"$\\lambda_\\mathrm{max}$\");\n",
    "    twin.fill_between(jnp.linspace(0,1,len(hess)), hess_mean - hess_std, hess_mean + hess_std, color=f\"C2\", alpha=.2, linewidth=0.);\n",
    "    \n",
    "    mainax.set_xlabel(\"Interpolation coefficient\");\n",
    "    mainax.set_ylabel(\"Angular error (degrees)\");\n",
    "    twin.set_ylabel(\"Top Hessian eigenvalue\");\n",
    "    mainax.set_xlim(0,1);\n",
    "    mainax.grid(True, linestyle=\"--\", linewidth=0.5, axis=\"both\");\n",
    "    fig.legend();\n",
    "    fig.savefig(f\"agg/interpolation_{name}.png\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
