{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_ops=true\"\n",
    "import optax, jax, json\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "from fedflax import train\n",
    "from models import LeNet\n",
    "from data import fetch_data\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce\n",
    "from utils import return_l2, opt_create, angle_err, nnx_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9db9f5",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpness-aware minimization optimizer\n",
    "def sam(model): return nnx.Optimizer(\n",
    "    model,\n",
    "    optax.contrib.sam(\n",
    "        optax.adamw(learning_rate=1e-3),\n",
    "        optax.chain(optax.contrib.normalize(), optax.adam(1e-2)),\n",
    "        sync_period=5\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "# Regular optimizer\n",
    "def opt_reg(model): return nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(learning_rate=1e-3),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "def partial_aggregate(models, alpha):\n",
    "    params, struct = jax.tree.flatten(nnx.to_tree(models))\n",
    "    avg_params = jax.tree.map(lambda p: jnp.mean(p, axis=0), params)\n",
    "    new_params = jax.tree.map(lambda p, ap: (1-alpha)*p + alpha*ap, params, avg_params)\n",
    "    return nnx.from_tree(jax.tree.unflatten(struct, new_params))\n",
    "\n",
    "# Calculate the dominant eigenvalue (lambda_max) of an nnx model using the power iteration method\n",
    "key = jax.random.key(42)\n",
    "@nnx.vmap(in_axes=(0,None,None,None,None,None))\n",
    "def lambda_max(model, x, z, y, max_iter=20, key=key):\n",
    "    # Convenience (jvp must act on a parameter-only pytree)\n",
    "    struct, theta, rest = nnx.split(model, (nnx.Param, nnx.BatchStat), ...)\n",
    "    reconstruct = lambda th: nnx.merge(struct, th, rest)\n",
    "\n",
    "    # Gradient of loss on this data\n",
    "    grad_fn = nnx.grad(lambda th: jnp.square(reconstruct(th)(x, z, train=True) - y).mean())\n",
    "\n",
    "    # Random normalized vector\n",
    "    def random_like(arr): nonlocal key; _, key = jax.random.split(key); return jax.random.normal(key, arr.shape)\n",
    "    v = jax.tree.map(lambda p: random_like(p), theta)\n",
    "    norm = nnx_norm(v)\n",
    "    v = v_prev = jax.tree.map(lambda v_: v_/norm, v)\n",
    "\n",
    "    # Perform iteration avoiding python bools\n",
    "    def true_fun(val):\n",
    "        # Power iteration step\n",
    "        v, _, i = val\n",
    "        hv = jax.jvp(\n",
    "            grad_fn,\n",
    "            (theta,),\n",
    "            (v,)\n",
    "        )[1]\n",
    "        norm = nnx_norm(hv)\n",
    "        v_new = jax.tree.map(lambda v_: v_/norm, hv)\n",
    "        return v_new, v, i+1\n",
    "    def cond_fun(val):\n",
    "        # Check convergence\n",
    "        v, v_prev, i = val\n",
    "        return jnp.logical_and(i<max_iter, jnp.logical_or(i==0, nnx_norm(v, v_prev)>=1e-3))\n",
    "    # Iterate\n",
    "    i0 = jnp.array(0)\n",
    "    v, *_ = jax.lax.while_loop(cond_fun, true_fun, (v, v_prev, i0))\n",
    "    # Return Rayleigh quotient (dot product between v and grad)\n",
    "    return jax.tree.reduce(\n",
    "        lambda acc, prod: acc+prod, \n",
    "        jax.tree.map(lambda v_, g_: jnp.sum(v_*g_), \n",
    "                     v, grad_fn(theta)\n",
    "                    )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c04db",
   "metadata": {},
   "source": [
    "## Interpolate between client and global, and measure metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get models after one-shot for various heterogeneity levels\n",
    "metrics = defaultdict(lambda: defaultdict(list))\n",
    "for opt_callable, beta, skew in [(opt_reg, 1., \"feature\"), (opt_reg, 0., \"overlap\"), (sam, 1., \"feature\"), (opt_reg, .01, \"feature\"), (opt_reg, .5, \"feature\"), (opt_reg, .5, \"overlap\"), (opt_reg, .99, \"overlap\"), (sam, .01, \"feature\"), (sam, .5, \"feature\")]:\n",
    "    # Train model\n",
    "    ds_train = fetch_data(skew=skew, beta=beta)\n",
    "    ds_val = fetch_data(skew=skew, beta=beta, partition=\"val\", batch_size=16)\n",
    "    ds_test = fetch_data(skew=skew, beta=beta, partition=\"test\", batch_size=16)\n",
    "    models, _ = train(LeNet(jax.random.key(42)), opt_callable, ds_train, return_l2(0.), ds_val, local_epochs=\"early\", rounds=1, max_patience=5, val_fn=angle_err)\n",
    "    # Interpolate\n",
    "    for alpha in tqdm(jnp.linspace(0,1,30), leave=False):\n",
    "        # Get the client models between global and local at alpha\n",
    "        models_agg = partial_aggregate(models, alpha)\n",
    "        # Average accuracy on local data\n",
    "        err_fn = nnx.jit(nnx.vmap(angle_err))\n",
    "        errs_l = reduce(lambda acc, b: acc + err_fn(models_agg,*b), ds_test, 0.) / len(ds_test)\n",
    "        metrics[f\"{skew}_{beta}_{opt_callable.__name__}\"][\"Avg. loc. err.\"].append(errs_l.tolist()) \n",
    "        # Average accuracy on global data\n",
    "        err_fn = nnx.jit(nnx.vmap(angle_err, in_axes=(0,None,None,None)))\n",
    "        errs_g = reduce(lambda acc, b: acc + err_fn(models_agg,b[0].reshape(-1,2),b[1].reshape(-1,36,60,1),b[2].reshape(-1,3)), ds_test, 0.) / len(ds_test)\n",
    "        metrics[f\"{skew}_{beta}_{opt_callable.__name__}\"][\"Avg. glob. err.\"].append(errs_g.tolist())\n",
    "        # Average top Hessian eigenvalue\n",
    "        y_test, x_test, z_test = reduce(lambda acc, b: (\n",
    "            jnp.concatenate([acc[0], b[0].reshape(-1,2)]), \n",
    "            jnp.concatenate([acc[1], b[1].reshape(-1,36,60,1)]), \n",
    "            jnp.concatenate([acc[2], b[2].reshape(-1,3)])\n",
    "        ), ds_test, (jnp.empty((0,2)), jnp.empty((0,36,60,1)), jnp.empty((0,3))))\n",
    "        lambda_maxs = lambda_max(models_agg, x_test, z_test, y_test)\n",
    "        metrics[f\"{skew}_{beta}_{opt_callable.__name__}\"][r\"Avg. $\\lambda_\\text{max}$\"].append(lambda_maxs.tolist())\n",
    "    # Save intermediate results\n",
    "    with open(\"agg/metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811288f",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering style\n",
    "plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "    \"font.sans-serif\": [\"Helvetica\"],\n",
    "    \"text.latex.preamble\": r\"\"\"\n",
    "        \\usepackage{amsmath, amssymb}\n",
    "        \\usepackage{mathptmx}  % Safe fallback for Times + math\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# Plot all settings\n",
    "metrics = json.load(open(\"agg/metrics.json\", \"r\"))\n",
    "for name in metrics.keys(): \n",
    "    # There are supposed to be three metrics per setting\n",
    "    fig, mainax = plt.subplots(dpi=300);\n",
    "    # Plot\n",
    "    loc = jnp.array(metrics[name][\"Avg. loc. err.\"]);\n",
    "    glob = jnp.array(metrics[name][\"Avg. glob. err.\"]);\n",
    "    mainax.plot(jnp.linspace(0,1,len(loc)), loc.mean(axis=-1), c=f\"C0\", label=\"Error on local data\");\n",
    "    mainax.plot(jnp.linspace(0,1,len(glob)), glob.mean(axis=-1), c=f\"C1\", label=\"Error on global data\");\n",
    "    mainax.fill_between(jnp.linspace(0,1,len(loc)), loc.min(axis=-1), loc.max(axis=-1), color=f\"C0\", alpha=.2, linewidth=0.);\n",
    "    mainax.fill_between(jnp.linspace(0,1,len(glob)), glob.min(axis=-1), glob.max(axis=-1), color=f\"C1\", alpha=.2, linewidth=0.);\n",
    "    mainax.set_ylabel(\"Angular error (degrees)\");\n",
    "    twin = mainax.twinx();\n",
    "    twin.set_ylabel(\"Top Hessian eigenvalue\");\n",
    "    hess = jnp.array(metrics[name][r\"Avg. $\\lambda_\\text{max}$\"]);\n",
    "    twin.plot(jnp.linspace(0,1,len(hess)), hess.mean(axis=-1), c=f\"C2\", label=r\"$\\lambda_\\mathrm{max}$\");\n",
    "    twin.fill_between(jnp.linspace(0,1,len(hess)), hess.min(axis=-1), hess.max(axis=-1), color=f\"C2\", alpha=.2, linewidth=0.);\n",
    "    mainax.set_xlabel(\"Interpolation coefficient\");\n",
    "    mainax.set_xlim(0,1);\n",
    "    mainax.grid(True, linestyle=\"--\", linewidth=0.5, axis=\"both\");\n",
    "    fig.legend();\n",
    "    fig.savefig(f\"agg/interpolation_{name}.png\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
